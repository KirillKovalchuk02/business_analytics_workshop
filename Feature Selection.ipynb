{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cbs = pd.read_excel(\"attachments/DATASET_DISEASES_CBS.xlsx\")\n",
    "df_gemeentes = pd.read_excel(\"attachments/DATASET_GEMENEENTES_PC_4.xlsx\")\n",
    "df_gezondheid = pd.read_excel(\"attachments/DATASET_GEZONDHEIDSMONITOR.xlsx\")\n",
    "df_housing = pd.read_excel(\"attachments/DATASET_INHABITANTS_HOUSING.xlsx\")\n",
    "df_orv_gem = pd.read_excel(\"attachments/DATASET_ORV_GEMEENTE.xlsx\")\n",
    "df_orv_pc = pd.read_excel(\"attachments/DATASET_ORV_PC.xlsx\")\n",
    "df_demo = pd.read_excel(\"attachments/DATASET_SOCIO_DEMO.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "dfs = {\"cbs\": df_cbs,\n",
    "       \"gemeentes\": df_gemeentes,\n",
    "       \"gezondheid\": df_gezondheid,\n",
    "       \"housing\": df_housing,\n",
    "       \"orv_gem\": df_orv_gem,\n",
    "       \"orv_pc\": df_orv_pc,\n",
    "       \"demo\": df_demo}\n",
    "\n",
    "\n",
    "\n",
    "#Function to parse special characters\n",
    "def decode_unicode(value):\n",
    "    if isinstance(value, str):\n",
    "        # Replace all occurrences of _xXXXX_ with the corresponding character\n",
    "        return re.sub(r'_x([0-9A-Fa-f]{4})_', lambda match: chr(int(match.group(1), 16)), value)\n",
    "    return value\n",
    "\n",
    "\n",
    "#General cleaning\n",
    "dfs_precleaned = {}\n",
    "for name, df in dfs.items():\n",
    "    df.columns = df.columns.str.lower() #standardized the names\n",
    "    df.rename(columns=lambda col: decode_unicode(col), inplace=True)\n",
    "    df = df.dropna(axis = 0, how = 'any') #dropped NaN\n",
    "    df = df.map(decode_unicode) #parsed special chars\n",
    "    df = df.rename({\"pc4\": \"postcode\"}, axis = 1) #standardize the key name to \"postcode\"\n",
    "    df = df.drop_duplicates()\n",
    "    try:\n",
    "        df['postcode'] = df['postcode'].astype(str).str.replace(\".0\",'') ##turn the postcode to string to use as join key\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    dfs_precleaned[name] = df\n",
    "\n",
    "\n",
    "#Table specific cleaning\n",
    "dfs_precleaned['cbs'] = dfs_precleaned['cbs'].drop(columns = \"diagnosis\")\n",
    "\n",
    "dfs_precleaned['gemeentes'] = dfs_precleaned['gemeentes'].drop(columns = \"gemnaam\")\n",
    "\n",
    "dfs_precleaned['housing'] = dfs_precleaned['housing'].drop([\"f34\", \"f35\"], axis=1)\n",
    "\n",
    "#ORV tables:\n",
    "orv_postcode = pd.merge(dfs_precleaned['gemeentes'], dfs_precleaned['orv_pc'], on = \"postcode\")\n",
    "orv_gemeente = dfs_precleaned['orv_gem'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the proportion of the population that are customers\n",
    "\n",
    "#gemeente level:\n",
    "orv_gemeente['customers_proportion'] = round(orv_gemeente.number_customers / orv_gemeente.inhabitants, 4)\n",
    "\n",
    "#pc level:\n",
    "orv_postcode['customers_proportion'] = round(orv_postcode.number_customers / orv_postcode.inhabitants, 4)\n",
    "\n",
    "#We can set some kind of threshold for proportion of customers to call the region \"under/over-represented\" and then\n",
    "#use it as a label for machine learning\n",
    "\n",
    "orv_pc_mean = round(orv_postcode['customers_proportion'].mean(), 5)\n",
    "orv_gem_mean = round(orv_gemeente['customers_proportion'].mean(), 5)\n",
    "orv_pc_std = round(orv_postcode['customers_proportion'].std(), 5)\n",
    "orv_gem_std = round(orv_gemeente['customers_proportion'].std(), 5)\n",
    "\n",
    "\n",
    "#Standardize the proportions\n",
    "\n",
    "orv_gemeente['customers_proportion_norm'] = orv_gemeente.customers_proportion.apply(lambda x: ((x - orv_gem_mean) / orv_gem_std))\n",
    "orv_postcode['customers_proportion_norm'] = orv_postcode.customers_proportion.apply(lambda x: ((x - orv_pc_mean) / orv_pc_std))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASSIFY THE OBSERVATIONS\n",
    "orv_gemeente['overrepresented'] = orv_gemeente['customers_proportion_norm'].apply(lambda x: 0 if x <= 0 else 1)\n",
    "orv_postcode['overrepresented'] = orv_postcode['customers_proportion_norm'].apply(lambda x: 0 if x <= 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO THE JOINS OF ORV DATA WITH OTHER TABLES TO GET ENRICHED DATASETS\n",
    "\n",
    "#Join with demo df\n",
    "demo_orv_pc = pd.merge(dfs_precleaned['demo'], orv_postcode, left_on = \"postcode\", right_on = \"postcode\")\n",
    "\n",
    "#Join with housing df\n",
    "housing = dfs_precleaned['housing'].copy()\n",
    "housing_orv_pc = pd.merge(housing, orv_postcode, left_on= 'postcode', right_on = 'postcode')\n",
    "\n",
    "\n",
    "#Join with gezondheid df\n",
    "\n",
    "gezondhd = dfs_precleaned['gezondheid'].copy()\n",
    "\n",
    "gezondheid_orv_pc = pd.merge(gezondhd, orv_postcode, left_on= 'postcode', right_on = 'postcode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE THE TABLE WITH ALL THE POSSIBLE FEATURES -> demo + housing + gezondheid + orv_pc\n",
    "orv_postcode_to_join = orv_postcode.add_suffix('_orv')\n",
    "orv_postcode_to_join = orv_postcode_to_join.rename(columns={\"overrepresented_orv\": \"overrepresented\"})\n",
    "#add suffices to track the original dataset of a variable\n",
    "demo_to_join = dfs_precleaned['demo'].add_suffix('_demo')\n",
    "house_to_join = dfs_precleaned['housing'].add_suffix('_housing')\n",
    "gezond_to_join = dfs_precleaned['gezondheid'].add_suffix('_gezond')\n",
    "\n",
    "\n",
    "#Remove/transform columns\n",
    "house_to_join['women_prop_housing'] = house_to_join['women_housing'] / house_to_join['total_housing']\n",
    "house_to_join = house_to_join.drop(['total_housing', 'women_housing', 'men_housing'], axis = 1)\n",
    "\n",
    "orv_total_join1 = pd.merge(demo_to_join, house_to_join, left_on = 'postcode_demo', right_on='postcode_housing')\n",
    "orv_total_join2 = pd.merge(orv_total_join1, gezond_to_join, left_on = 'postcode_housing', right_on='postcode_gezond')\n",
    "orv_total_join = pd.merge(orv_total_join2, orv_postcode_to_join, left_on = 'postcode_housing', right_on='postcode_orv')\n",
    "\n",
    "\n",
    "to_drop_orv = [col for col in orv_total_join.columns if '_orv' in col]\n",
    "\n",
    "to_drop_final = to_drop_orv + [\"year_demo\", \"postcode_demo\", \"nr_households_demo\", \"postcode_housing\", \"postcode_gezond\"]\n",
    "\n",
    "orv_total_join_without_orv = orv_total_join.drop(to_drop_final, axis=1)\n",
    "\n",
    "\n",
    "#ALTERNATIVE CLASSIFICATIONS:\n",
    "#Multi-class with under, over and average (std)\n",
    "\n",
    "to_drop_final.remove('customers_proportion_norm_orv')\n",
    "orv_total_join_alter = orv_total_join.drop(to_drop_final, axis=1)\n",
    "\n",
    "orv_total_join_alter['overrepresented_alt'] = orv_total_join_alter['customers_proportion_norm_orv'].apply( ####2 for over, 1 for under, 0 for normal\n",
    "   lambda x: 2 if x > 1 else (1 if x < -1 else 0)\n",
    ")\n",
    "\n",
    "orv_total_join_alter = orv_total_join_alter.drop([\"customers_proportion_norm_orv\", \"overrepresented\"], axis = 1)\n",
    "orv_total_join_alter = orv_total_join_alter.rename(columns={\"overrepresented_alt\": \"overrepresented\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "cols = list(orv_total_join.columns)\n",
    "\n",
    "cols.remove(\"municipality_orv\")\n",
    "\n",
    "\n",
    "orv_total_join_standardized = pd.DataFrame(scaler.fit_transform(orv_total_join.drop(columns=\"municipality_orv\")), columns=cols)\n",
    "\n",
    "df = pd.concat([orv_total_join['municipality_orv'], orv_total_join_standardized], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstandardized_df = orv_total_join.drop(columns=df.filter(like='postcode').columns)\n",
    "\n",
    "\n",
    "df_grouped = unstandardized_df.groupby('municipality_orv').mean()\n",
    "df_grouped = df_grouped.sort_values(by = \"customers_proportion_norm_orv\")\n",
    "\n",
    "df_bot = df_grouped.head(20)\n",
    "df_top = df_grouped.tail(20)\n",
    "\n",
    "dict_of_res = dict()\n",
    "for col in df_grouped.columns:\n",
    "    diff = abs(round(df_bot[col].mean(), 4) - round(df_top[col].mean(), 4))\n",
    "    std = df_grouped[col].std()\n",
    "    how_many = float(round(diff / std, 2))\n",
    "    dict_of_res[col] = how_many\n",
    "\n",
    "\n",
    "\n",
    "print(round(df_bot['age_from_65_housing'].mean(), 4))\n",
    "print(round(df_top['age_from_65_housing'].mean(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(dict_of_res.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "sorted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*FEATURE SELECTION*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions for ML dataset prep\n",
    "def scale_dataset(dataframe, to_drop_additional = [], oversample=False):\n",
    "  to_drop = ['year', 'postcode',\t'nr_households', 'municipality',\n",
    "                                'heavily_overrepresented_orv', 'heavily_underrepresented_orv',\n",
    "                                \"customers_proportion\", \"customers_proportion_norm\", \"total\",\n",
    "                                'inhabitants', 'number_customers'] + to_drop_additional\n",
    "  for col in to_drop:\n",
    "    try:\n",
    "      dataframe = dataframe.drop(col, axis=1)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "\n",
    "  X = dataframe[dataframe.columns[:-1]].values\n",
    "  y = dataframe[dataframe.columns[-1]].values\n",
    "\n",
    "  X_df = dataframe[dataframe.columns[:-1]]\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  X = scaler.fit_transform(X)\n",
    "\n",
    "  if oversample:\n",
    "    ros = RandomOverSampler()\n",
    "    X, y = ros.fit_resample(X, y)\n",
    "\n",
    "  data = np.hstack((X, np.reshape(y, (-1, 1))))\n",
    "\n",
    "  return data, X, y, X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the CV\n",
    "data_cv, X_cv, Y_cv, X_df = scale_dataset(orv_total_join_alter, oversample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE IMPORTANCE\n",
    "\n",
    "#RF MODEL\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=400,       \n",
    "    max_depth=4,            \n",
    "    random_state=42,        \n",
    "    #class_weight='balanced' \n",
    ")\n",
    "\n",
    "rf_model.fit(X_cv, Y_cv)\n",
    "\n",
    "\n",
    "#To differentiate between binary and multinomial log reg\n",
    "if len(list(set(Y_cv))) == 2:\n",
    "    lg_model = LogisticRegression()\n",
    "else:\n",
    "    lg_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "lg_model.fit(X_cv, Y_cv)\n",
    "\n",
    "\n",
    "#BASE RF FEATURE IMPORTANCES\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "importance_rf_df = pd.DataFrame({\n",
    "    'Feature': X_df.columns,\n",
    "    'Importance_RF': feature_importances\n",
    "}).sort_values(by='Importance_RF', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "#PERMUTATION IMPORTANCE\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_result_rf = permutation_importance(rf_model, X_cv, Y_cv, n_repeats=10, random_state=42) #Based on random forest\n",
    "perm_result_logreg = permutation_importance(lg_model, X_cv, Y_cv, n_repeats=10, random_state=42) #Based on log reg, either binomial or multinomial\n",
    "\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': X_df.columns,\n",
    "    'Importance_Perm_RF': abs(perm_result_rf.importances_mean),\n",
    "    'Importance_Perm_LR': abs(perm_result_logreg.importances_mean)\n",
    "}).sort_values(by='Importance_Perm_RF', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "#RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# With Logistic Regression as the estimator\n",
    "lg_model = LogisticRegression(**lg_model.get_params()) #this resets the model so we can reuse it for RFE\n",
    "\n",
    "estimator_logreg = lg_model\n",
    "selector_logreg = RFE(estimator_logreg, n_features_to_select=30)\n",
    "selector_logreg.fit(X_cv, Y_cv)\n",
    "\n",
    "selected_vars_rfe_logreg = list(X_df.columns[selector_logreg.support_])\n",
    "\n",
    "estimator_rf = RandomForestClassifier()\n",
    "selector_rf = RFE(estimator_rf, n_features_to_select=30)\n",
    "selector_rf.fit(X_cv, Y_cv)\n",
    "\n",
    "selected_vars_rfe_rf = list(X_df.columns[selector_rf.support_])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_vars_rfe_logreg)\n",
    "\n",
    "print(selected_vars_rfe_rf)\n",
    "\n",
    "#[x for x in selected_vars_rfe_rf if x in selected_vars_rfe_logreg]\n",
    "\n",
    "#[x for x in selected_vars_rfe_rf if x not in selected_vars_rfe_logreg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.merge(importance_rf_df, perm_importance_df, on = 'Feature')\n",
    "#importance = importance.rename(columns={\"Importance_x\": \"importance_random_forest\", \"Importance_y\": \"importance_permutation_method\"})\n",
    "\n",
    "#normalize both importance scores\n",
    "scaler_new = StandardScaler()\n",
    "importance['importance_rf_norm'] = scaler_new.fit_transform(importance[['Importance_RF']])\n",
    "importance['importance_perm_rf_norm'] = scaler_new.fit_transform(importance[['Importance_Perm_RF']])\n",
    "importance['importance_perm_lr_norm'] = scaler_new.fit_transform(importance[['Importance_Perm_LR']])\n",
    "\n",
    "importance['importance_avg'] = (importance['importance_rf_norm'] + importance['importance_perm_rf_norm'] + importance['importance_perm_lr_norm']) / 3\n",
    "importance = importance.sort_values(by = \"importance_avg\", ascending=False)\n",
    "\n",
    "main_vars_importance = list(importance.head(30)['Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=X_df.shape[1])  # Use all components initially\n",
    "pca.fit(X_cv)\n",
    "\n",
    "\n",
    "loadings = pca.components_.T \n",
    "columns = X_df.columns\n",
    "\n",
    "# Create a DataFrame to analyze loadings\n",
    "loading_df = pd.DataFrame(loadings, columns=[f'PC{i+1}' for i in range(len(pca.components_))], index=columns)\n",
    "\n",
    "important_vars_pc1 = loading_df['PC1'].abs().sort_values(ascending=False)\n",
    "\n",
    "main_vars_pca = list(important_vars_pc1.index[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = main_vars_pca + main_vars_importance + selected_vars_rfe_logreg + selected_vars_rfe_rf\n",
    "\n",
    "#for var in unique_features:\n",
    "counter = Counter(all_features)\n",
    "selected_features = list({i for i in counter if counter[i] >= 3}) #A \"consensus\" filter, i.e. we leave the feature in only if it is present in at least 2 selection out of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECT THE FEATURES AND RUN NORMAL LOG REG\n",
    "\n",
    "\n",
    "def run_log_reg(dataframe, multi=False):\n",
    "    X_reg = dataframe[selected_features]\n",
    "    y_reg = dataframe.iloc[:,-1]\n",
    "    y_reg = y_reg.astype('category').cat.reorder_categories([0, 1, 2], ordered=True).cat.codes #explicitly set 0 (\"Normal\") as reference cat\n",
    "\n",
    "    if multi:\n",
    "        y_reg_encoded = y_reg.astype('category').cat.codes\n",
    "        log_reg_multi = sm.MNLogit(y_reg_encoded, X_reg).fit()\n",
    "        to_drop = ['t', \"[0.025\", \"0.975]\", \"Std.Err.\"]\n",
    "\n",
    "        results_reg1 = log_reg_multi.summary2().tables[1]\n",
    "        results_reg1 = results_reg1.drop(columns = to_drop)\n",
    "\n",
    "        results_reg2 = log_reg_multi.summary2().tables[2]\n",
    "        results_reg2 = results_reg2.drop(columns = to_drop)\n",
    "\n",
    "        results_reg2.columns = results_reg2.columns + \"_2\"\n",
    "        results_reg = pd.concat([results_reg1, results_reg2], axis = 1)\n",
    "    else:\n",
    "        log_reg_binary = sm.Logit(y_reg, X_reg).fit()\n",
    "        results_reg = log_reg_binary.summary2().tables[1]\n",
    "\n",
    "    return results_reg\n",
    "\n",
    "\n",
    "\n",
    "results_df_lr = run_log_reg(orv_total_join_alter, multi=True)\n",
    "results_df_lr = results_df_lr.rename(columns={\"y = 0\": \"Underrepresented vs Normal\",\n",
    "                                              \"y = 1_2\": \"Overrepresented vs Normal\",\n",
    "                                              \"P>|t|\": \"p-value\",\n",
    "                                              \"P>|t|_2\": \"p-value2\"})\n",
    "\n",
    "results_df_lr['odds_ratio_y=1'] = pow(math.e, results_df_lr['Coef.'])\n",
    "results_df_lr['odds_ratio_y=2'] = pow(math.e, results_df_lr['Coef._2'])\n",
    "\n",
    "results_df_lr_filtered = results_df_lr[(results_df_lr['p-value'] < 0.05) | (results_df_lr['p-value2'] < 0.05)]\n",
    "\n",
    "results_df_lr_filtered = results_df_lr_filtered.loc[results_df_lr_filtered['Underrepresented vs Normal'] != \"ses_score_av_demo\"]\n",
    "\n",
    "results_df_lr_filtered['odds_ratio_y=1'] = np.exp(results_df_lr_filtered['Coef.'])\n",
    "results_df_lr_filtered['odds_ratio_y=2'] = np.exp(results_df_lr_filtered['Coef._2'])\n",
    "\n",
    "results_df_lr_filtered['Underrepresented vs Normal'] = results_df_lr_filtered['Underrepresented vs Normal'].str.replace(r\"_[A-Za-z0-9]+$\", '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(results_df_lr_filtered['Underrepresented vs Normal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = results_df_lr_filtered.set_index('Underrepresented vs Normal')[['odds_ratio_y=1']].plot(kind='bar', figsize=(10, 6))\n",
    "\n",
    "plt.axhline(y=1, color='gray', linestyle='--')\n",
    "plt.ylabel('Odds Ratio')\n",
    "plt.title('Odds Ratios for Underrepresented vs Normal representation (with SES score removed) ')\n",
    "plt.legend().remove()\n",
    "\n",
    "for rect in ax.patches:\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height(),\n",
    "            f'{rect.get_height():.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_lr_filtered = results_df_lr[results_df_lr.iloc[:,-1] < 0.05]\n",
    "results_df_lr_filtered.sort_values(by = list(results_df_lr_filtered.columns)[-1], ascending=False)\n",
    "\n",
    "vars_selected_final = list(results_df_lr_filtered.index)\n",
    "vars_selected_final.append('overrepresented')\n",
    "\n",
    "ml_train_data = orv_total_join_alter[vars_selected_final]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*MACHINE LEARNING MODELS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml, X_ml, Y_ml, X_df_ml = scale_dataset(ml_train_data, oversample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
    "\n",
    "scoring_binary = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "scoring_multinomial = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted')\n",
    "}\n",
    "\n",
    "3\n",
    "\n",
    "rf_model_new = RandomForestClassifier(\n",
    "    n_estimators=700,       # Number of trees in the forest\n",
    "    max_depth=3,            # Maximum depth of each tree (None = fully grown trees)\n",
    "    #random_state=42,        # For reproducibility\n",
    ")\n",
    "#rf_model_new.fit(X_ml, Y_ml)\n",
    "cv_results_rf = cross_validate(rf_model_new, X_ml, Y_ml, cv=5, scoring=scoring_multinomial, return_train_score=True)\n",
    "\n",
    "# for metric in scoring_binary:\n",
    "#     print(f\"{metric.capitalize()} (Mean): {np.mean(cv_results_rf[f'test_{metric}']):.2f}\")\n",
    "\n",
    "for metric in scoring_multinomial.keys():\n",
    "    print(f\"{metric.capitalize()} (Mean): {np.mean(cv_results_rf[f'test_{metric}']):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOG REG\n",
    "\n",
    "if len(list(set(Y_ml))) == 2:\n",
    "    lg_model = LogisticRegression()\n",
    "else:\n",
    "    lg_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results_logistic = cross_validate(lg_model, X_ml, Y_ml, cv=5, scoring=scoring_multinomial, return_train_score=True)\n",
    "\n",
    "for metric in scoring_multinomial:\n",
    "    print(f\"{metric.capitalize()} (Mean): {np.mean(cv_results_logistic[f'test_{metric}']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='poly', random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results_svm = cross_validate(svm_model, X_ml, Y_ml, cv=5, scoring=scoring_multinomial, return_train_score=True)\n",
    "\n",
    "# Print mean scores for each metric\n",
    "for metric in scoring_multinomial:\n",
    "    print(f\"{metric.capitalize()} (Mean): {np.mean(cv_results_svm[f'test_{metric}']):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
